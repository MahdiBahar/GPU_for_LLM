After installing vllm and pytorch we should download models from huggingface 
For this purpose I use this code:

from huggingface_hub import snapshot_download
# This downloads to ~/.cache/huggingface/hub
snapshot_download("microsoft/phi-4-mini-instruct", cache_dir="~/.cache/vllm")

#####It doesnt work

Then I went to serve the model from terminal to download it in local system. It seems that this action happens when we serve the model for the first time.

** Serve directly from HF by repo ID

vllm serve microsoft/phi-4-mini-instruct
(vLLM will download the model under ~/.cache/vllm/microsoft_phi-4-mini-instruct on first use.)


##### I got error about running out of GPU RAM because the model is being loaded in full bfloat16 precision (which for even a 1 B–1.3 B‑parameter model can easily hit 8 GB).
 The easiest fix is to use 4‑bit quantization, which will shrink the weights on‑GPU by roughly 4×.

 ====> So, I use this library:

 pip install "bitsandbytes>=0.46.1"

 then serve the model in vLLM:

 vllm serve microsoft/phi-4-mini-instruct --quantization bitsandbytes


I got the error about that with the default context length (131 072 tokens) you simply don’t have enough KV‑cache memory on your GPU:

To serve at least one request with the model’s max seq len (131072), 16 GiB KV cache is needed, which is larger than the available KV cache memory (2.24 GiB). … Try … decreasing max_model_len when initializing the engine.

You have two knobs to turn:

1_ Reduce the maximum context length

vllm serve microsoft/phi-4-mini-instruct \
  --quantization bitsandbytes \
  --max-model-len 8192

If you know you’ll never need more than 4 k tokens, you can go even lower:

--max-model-len 4096

2_ Lower vLLM’s pre‑allocation fraction

vLLM by default pre‑allocates 90% of your GPU’s RAM to avoid fragmentation. You can lower that to, say, 70%:


vllm serve microsoft/phi-4-mini-instruct \
  --quantization bitsandbytes \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.7
This tells vLLM to only grab 70% of your GPU memory for its own pools, leaving a buffer so CUDA Graphs or other processes don’t trigger fragmentation.

Finally:

export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
vllm serve microsoft/phi-4-mini-instruct \
  --quantization bitsandbytes \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.7

modify the above command to solve this error
(The OOM is happening during the dummy–request warm‑up (256 sequences by default), which allocates a huge sampling buffer. 
You can fix it by telling vLLM to warm up with far fewer sequences and/or grab less of your GPU’s RAM.)
:

export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

vllm serve microsoft/phi-4-mini-instruct \
  --quantization bitsandbytes \
  --max-model-len 4096 \
  --gpu-memory-utilization 0.6 \
  --max-num-seqs 1

It is done................

Note: the address which model is downloaded there is: 

/home/mahdi/.cache/huggingface/hub


We have an important issue: The model should be in .cache/vllm ,however, it located in huggingface/hub.
So, I should change the direction:
HF_ROOT=~/.cache/huggingface/hub
MODEL_DIR=models--microsoft--phi-4-mini-instruct
SNAPSHOT=$(ls $HF_ROOT/$MODEL_DIR/snapshots)

In this part we link the vllm address to snapshots in huggingface hub.
ln -s   $HF_ROOT/$MODEL_DIR/snapshots/$SNAPSHOT   ~/.cache/vllm/microsoft_phi-4-mini-instruct

Because the model in huggingface is linked to blobs in there we should handle this too.

ln -s    /home/mahdi/.cache/huggingface/hub/models--microsoft--phi-4-mini-instruct/blobs  ~/.cache/blobs

*** Now, we should serve the model locally:
# Using “flag=value” avoids the space issue altogether:
vllm serve ~/.cache/vllm/microsoft_phi-4-mini-instruct \
  --quantization=bitsandbytes \
  --max-model-len=4096 \
  --gpu-memory-utilization=0.6 \
  --max-num-seqs=1


Now, the model is up, e should test it.



