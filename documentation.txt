After installing vllm and pytorch we should download models from huggingface 
For this purpose I use this code:

from huggingface_hub import snapshot_download
# This downloads to ~/.cache/huggingface/hub
snapshot_download("microsoft/phi-4-mini-instruct", cache_dir="~/.cache/vllm")

#####It doesnt work

Then I went to serve the model from terminal to download it in local system. It seems that this action happens when we serve the model for the first time.

** Serve directly from HF by repo ID

vllm serve microsoft/phi-4-mini-instruct
(vLLM will download the model under ~/.cache/vllm/microsoft_phi-4-mini-instruct on first use.)


##### I got error about running out of GPU RAM because the model is being loaded in full bfloat16 precision (which for even a 1 B–1.3 B‑parameter model can easily hit 8 GB).
 The easiest fix is to use 4‑bit quantization, which will shrink the weights on‑GPU by roughly 4×.

 ====> So, I use this library:

 pip install "bitsandbytes>=0.46.1"

 then serve the model in vLLM:

 vllm serve microsoft/phi-4-mini-instruct --quantization bitsandbytes


I got the error about that with the default context length (131 072 tokens) you simply don’t have enough KV‑cache memory on your GPU:

To serve at least one request with the model’s max seq len (131072), 16 GiB KV cache is needed, which is larger than the available KV cache memory (2.24 GiB). … Try … decreasing max_model_len when initializing the engine.

You have two knobs to turn:

1_ Reduce the maximum context length

vllm serve microsoft/phi-4-mini-instruct \
  --quantization bitsandbytes \
  --max-model-len 8192

If you know you’ll never need more than 4 k tokens, you can go even lower:

--max-model-len 4096

2_ Lower vLLM’s pre‑allocation fraction

vLLM by default pre‑allocates 90% of your GPU’s RAM to avoid fragmentation. You can lower that to, say, 70%:


vllm serve microsoft/phi-4-mini-instruct \
  --quantization bitsandbytes \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.7
This tells vLLM to only grab 70% of your GPU memory for its own pools, leaving a buffer so CUDA Graphs or other processes don’t trigger fragmentation.

Finally:

export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
vllm serve microsoft/phi-4-mini-instruct \
  --quantization bitsandbytes \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.7

modify the above command to solve this error
(The OOM is happening during the dummy–request warm‑up (256 sequences by default), which allocates a huge sampling buffer. 
You can fix it by telling vLLM to warm up with far fewer sequences and/or grab less of your GPU’s RAM.)
:

export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

vllm serve microsoft/phi-4-mini-instruct \
  --quantization bitsandbytes \
  --max-model-len 4096 \
  --gpu-memory-utilization 0.6 \
  --max-num-seqs 1

It is done................

Note: the address which model is downloaded there is: 

/home/mahdi/.cache/huggingface/hub


We have an important issue: The model should be in .cache/vllm ,however, it located in huggingface/hub.
So, I should change the direction:
HF_ROOT=~/.cache/huggingface/hub
MODEL_DIR=models--microsoft--phi-4-mini-instruct
SNAPSHOT=$(ls $HF_ROOT/$MODEL_DIR/snapshots)

In this part we link the vllm address to snapshots in huggingface hub.
ln -s   $HF_ROOT/$MODEL_DIR/snapshots/$SNAPSHOT   ~/.cache/vllm/microsoft_phi-4-mini-instruct

Because the model in huggingface is linked to blobs in there we should handle this too.

ln -s    /home/mahdi/.cache/huggingface/hub/models--microsoft--phi-4-mini-instruct/blobs  ~/.cache/blobs

*** Now, we should serve the model locally:
# Using “flag=value” avoids the space issue altogether:
vllm serve ~/.cache/vllm/microsoft_phi-4-mini-instruct \
  --quantization=bitsandbytes \
  --max-model-len=4096 \
  --gpu-memory-utilization=0.6 \
  --max-num-seqs=1


Now, the model is up, we should test it.

run this on terminal:
uvicorn fastapi_vLLM:app --host 0.0.0.0 --port 8000

Check the system from fastapi localhost address:
http://localhost:8000/docs

The system works properly. 

Now, the issue is that how can handle concurrency?

for this reseon, I created a new .py code to handle this but it doesnt answer properly when we sent 2 request from two different devices.

To solve this, chatgpt suggested that run the server with these commands:

export VLLM_LOG_STATS=1
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export HF_HUB_OFFLINE=1
export TRANSFORMERS_OFFLINE=1

vllm serve   ~/.cache/huggingface/hub/models--microsoft--phi-4-mini-instruct/snapshots/5a149550068a1eb93398160d8953f5f56c3603e9   
--quantization bitsandbytes   --max-model-len 4096   --enable-chunked-prefill   --max-num-batched-tokens 2048   --max-num-seqs 4
    --gpu-memory-utilization 0.5 --host 0.0.0.0 --port 8000


  It seems that this way is working..


Now, I want to send a lot of request to test oncurency. For this reason, I install hey to see this issue 
and then when my server is up, I run this code to see the perforance of the model in front of concurrency.

hey   -n 40 -c 10  -m POST   -H "Content-Type: application/json"   
-d '{"prompt":"Hello. What is your idea about quantum computing and its related effects on human life?","max_tokens":20,"temperature":0.7}'  
 http://localhost:8000/v1/completions

